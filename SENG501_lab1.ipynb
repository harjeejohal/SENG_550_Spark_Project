{"cells":[{"cell_type":"markdown","source":["# ![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n **Exploratory Analysis of Deerfoot Trail Commute Times**\n#### This lab will build on the techniques covered in the Spark tutorial to develop a simple application to compute some stats on commute times on Deerfoot Trail.  We will use the commute times and accidents data collected for Deerfoot Trail for the period September 2013 to April 2014.\n#### ** During this lab we will cover: **\n#### *Part 1:* Creating a base RDD and pair RDDs\n#### *Part 2:* Counting with pair RDDs\n#### *Part 3:* Finding mean values\n#### *Part 4:* Compute basic stats about the Deerfoot Trail data\n#### Note that, for reference, you can look up the details of the relevant methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkContext\nsc = SparkContext()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-2329435937481641&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansigreen\">from</span> pyspark <span class=\"ansigreen\">import</span> SparkContext<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>sc <span class=\"ansiyellow\">=</span> SparkContext<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansicyan\">__init__</span><span class=\"ansiblue\">(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)</span>\n<span class=\"ansigreen\">    113</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    114</span>         self<span class=\"ansiyellow\">.</span>_callsite <span class=\"ansiyellow\">=</span> first_spark_call<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">or</span> CallSite<span class=\"ansiyellow\">(</span>None<span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">,</span> None<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 115</span><span class=\"ansiyellow\">         </span>SparkContext<span class=\"ansiyellow\">.</span>_ensure_initialized<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> gateway<span class=\"ansiyellow\">=</span>gateway<span class=\"ansiyellow\">,</span> conf<span class=\"ansiyellow\">=</span>conf<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    116</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    117</span>             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.py</span> in <span class=\"ansicyan\">_ensure_initialized</span><span class=\"ansiblue\">(cls, instance, gateway, conf)</span>\n<span class=\"ansigreen\">    297</span>                         <span class=\"ansiblue\">&quot; created by %s at %s:%s &quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    298</span>                         % (currentAppName, currentMaster,\n<span class=\"ansigreen\">--&gt; 299</span><span class=\"ansiyellow\">                             callsite.function, callsite.file, callsite.linenum))\n</span><span class=\"ansigreen\">    300</span>                 <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    301</span>                     SparkContext<span class=\"ansiyellow\">.</span>_active_spark_context <span class=\"ansiyellow\">=</span> instance<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /tmp/1575328904531-0/PythonShell.py:971 </div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["### ** Part 1: Creating a base RDD and pair RDDs **"],"metadata":{}},{"cell_type":"markdown","source":["#### In this part of the lab, we will explore creating a base RDD with `parallelize` and using pair RDDs to count words."],"metadata":{}},{"cell_type":"markdown","source":["#### ** (1a) Create a base RDD **\n#### We'll start by generating a base RDD by using a Python list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD."],"metadata":{}},{"cell_type":"code","source":["daysList = ['sunday', 'monday', 'tuesday', 'tuesday', 'friday']\ndaysRDD = sc.parallelize(daysList, 4)\n# Print out the type of daysRDD\nprint type(daysRDD)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">&lt;class &apos;pyspark.rdd.RDD&apos;&gt;\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["#### ** (1b) Pluralize and test **\n#### Let's use a `map()` transformation to add the letter 's' to each string in the base RDD we just created. We'll define a Python function that returns the word with an 's' at the end of the word.  Please replace `<FILL IN>` with your solution.  The print statement is a test of your function.\n#### This is the general form that exercises will take.  Exercises will include an explanation of what is expected, followed by code cells where one cell will have one or more `<FILL IN>` sections.  The cell that needs to be modified will have `# TODO: Replace <FILL IN> with appropriate code` on its first line."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndef makePlural(word):\n    \"\"\"Adds an 's' to `word`.\n\n    Note:\n        This is a simple function that only adds an 's'.  No attempt is made to follow proper\n        pluralization rules.\n\n    Args:\n        word (str): A string.\n\n    Returns:\n        str: A string with 's' added to it.\n    \"\"\"\n    word = \"{}s\".format(word)\n    return word\n    \n\nprint makePlural('sunday')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sundays\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["#### ** (1c) Apply `makePlural` to the base RDD **\n#### Now pass each item in the base RDD into a [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) transformation that applies the `makePlural()` function to each element. And then call the [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) action to see the transformed RDD."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\npluralRDD = daysRDD.map(makePlural)\nprint pluralRDD.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&apos;sundays&apos;, &apos;mondays&apos;, &apos;tuesdays&apos;, &apos;tuesdays&apos;, &apos;fridays&apos;]\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["#### ** (1d) Pass a `lambda` function to `map` **\n#### Let's create the same RDD using a `lambda` function."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\npluralLambdaRDD = daysRDD.map(lambda singular: \"{}s\".format(singular))\nprint pluralLambdaRDD.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&apos;sundays&apos;, &apos;mondays&apos;, &apos;tuesdays&apos;, &apos;tuesdays&apos;, &apos;fridays&apos;]\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["#### ** (1e) Length of each word **\n#### Now use `map()` and a `lambda` function to return the first character in each word.  We'll `collect` this result directly into a variable."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\npluralFirstChars = (pluralRDD.\n                 map(lambda day: day[0])\n                 .collect())\nprint pluralFirstChars"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&apos;s&apos;, &apos;m&apos;, &apos;t&apos;, &apos;t&apos;, &apos;f&apos;]\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["#### ** (1f) Pair RDDs **\n#### Often we would need to deal with pair RDDs.  A pair RDD is an RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, we will create a pair consisting of `('<day>', 1)` for each word element in the RDD.\n#### We can create the pair RDD using the `map()` transformation with a `lambda()` function to create a new RDD."],"metadata":{}},{"cell_type":"code","source":["map# TODO: Replace <FILL IN> with appropriate code\ndayPairs = daysRDD.map(lambda day: (day, 1))\nprint dayPairs.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&apos;sunday&apos;, 1), (&apos;monday&apos;, 1), (&apos;tuesday&apos;, 1), (&apos;tuesday&apos;, 1), (&apos;friday&apos;, 1)]\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["### ** Part 2: Counting with pair RDDs **"],"metadata":{}},{"cell_type":"markdown","source":["#### Now, let's count the number of times a particular day appears in the RDD. There are multiple ways to perform the counting, but some are much less efficient than others.\n#### A naive approach would be to `collect()` all of the elements and count them in the driver program. While this approach could work for small datasets, we want an approach that will work for any size dataset including terabyte- or petabyte-sized datasets. In addition, performing all of the work in the driver program is slower than performing it in parallel in the workers. For these reasons, we will use data parallel operations."],"metadata":{}},{"cell_type":"markdown","source":["#### ** (2a) `groupByKey()` approach **\n#### An approach you might first consider (we'll see shortly that there are better ways) is based on using the [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) transformation. As the name implies, the `groupByKey()` transformation groups all the elements of the RDD with the same key into a single list in one of the partitions. There are two problems with using `groupByKey()`:\n  + #### The operation requires a lot of data movement to move all the values into the appropriate partitions.\n  + #### The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory in a worker.\n \n#### Use `groupByKey()` to generate a pair RDD of type `('day', iterator)`."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n# Note that groupByKey requires no parameters\ndaysGrouped = dayPairs.groupByKey()\nfor key, value in daysGrouped.collect():\n    print '{0}: {1}'.format(key, list(value))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sunday: [1]\ntuesday: [1, 1]\nmonday: [1]\nfriday: [1]\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["#### ** (2b) Use `groupByKey()` to obtain the counts **\n#### Using the `groupByKey()` transformation creates an RDD containing 3 elements, each of which is a pair of a day and a Python iterator.\n#### Now sum the iterator using a `map()` transformation.  The result should be a pair RDD consisting of (day, count) pairs."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndayCountsGrouped = daysGrouped.map(lambda pair: (pair[0], sum(pair[1])))\nprint dayCountsGrouped.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&apos;sunday&apos;, 1), (&apos;tuesday&apos;, 2), (&apos;monday&apos;, 1), (&apos;friday&apos;, 1)]\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["#### ** (2c) Counting using `reduceByKey` **\n#### A better approach is to start from the pair RDD and then use the [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) transformation to create a new pair RDD. The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\n# Note that reduceByKey takes in a function that accepts two values and returns a single value\ndayCounts = dayPairs.reduceByKey(lambda a, b: a + b)\nprint dayCounts.collect()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&apos;sunday&apos;, 1), (&apos;tuesday&apos;, 2), (&apos;monday&apos;, 1), (&apos;friday&apos;, 1)]\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["#### ** (2d) All together **\n#### The expert version of the code performs the `map()` to pair RDD, `reduceByKey()` transformation, and `collect` in one statement."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndayCountsCollected = (daysRDD.\n                       map(lambda day: (day, 1)).\n                       reduceByKey(lambda x, y: x + y).\n                       collect())\nprint dayCountsCollected"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&apos;sunday&apos;, 1), (&apos;tuesday&apos;, 2), (&apos;monday&apos;, 1), (&apos;friday&apos;, 1)]\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["### ** Part 3: Finding unique days and a mean value **"],"metadata":{}},{"cell_type":"markdown","source":["#### ** (3a) Unique words **\n#### Calculate the number of unique days in `daysRDD`.  You can use other RDDs that you have already created to make this easier."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nuniqueDays = len(dayCountsCollected)\nprint uniqueDays"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">4\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["#### ** (3b) Mean using `reduce` **\n#### Find the mean number of days per unique day in `dayCounts`.\n#### Use a `reduce()` action to sum the counts in `dayCounts` and then divide by the number of unique days.  First `map()` the pair RDD `dayCounts`, which consists of (key, value) pairs, to an RDD of values."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\nfrom operator import add\ntotalCount = (dayCounts\n              .map(lambda pair: pair[1])\n              .reduce(lambda a, b: a + b))\naverage = totalCount / float(uniqueDays)\nprint totalCount\nprint round(average, 2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">5\n1.25\n</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["### ** Part 4: Compute Deerfoot Trail stats **"],"metadata":{}},{"cell_type":"markdown","source":["#### In this section we will apply some of the above concepts towards analyzing commute time and accidents data collected for Deerfoot Trail."],"metadata":{}},{"cell_type":"markdown","source":["#### ** (4a) Loading the data **\n#### We will first load the data.  The data was collected in the period September 2013 to April 2014.  It was obtained by querying Google Maps for commute times and Twitter for accident reports.  Although this data set is very small, because we are using parallel computation via Spark the functions we develop will scale for larger data sets.  To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We will use `take(15)` to print 15 lines from this file."],"metadata":{}},{"cell_type":"code","source":["# Just run this code\nimport os.path\nbaseDir = os.path.join('data')\ninputPath = os.path.join('SENG501', 'lab1', 'deerfoot.csv')\nfileName = 'deerfoot.csv'\n\n# provide correct full path of file\ndeerfootRDD = (sc.textFile(\"/FileStore/tables/deerfoot.csv\", 8))\nprint '\\n'.join(deerfootRDD.zipWithIndex().map(lambda (l, num): '{0}: {1}'.format(num, l)).take(15))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0: 21/09/2013,Saturday,34,34,34,34,35,34,35,36,38,36,36,35,35,35,35,35,36,34,34,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2\n1: 22/09/2013,Sunday,34,34,34,34,34,34,34,35,35,35,34,35,34,35,34,34,34,34,34,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3\n2: 23/09/2013,Monday,35,36,41,43,45,41,36,35,35,35,37,40,43,46,43,37,34,34,35,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,2\n3: 24/09/2013,Tuesday,35,36,40,44,52,41,38,36,36,36,37,40,44,47,42,39,34,35,35,0,0,0,1,1,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,4,1,1,0,5\n4: 25/09/2013,Wednesday,35,36,40,39,39,37,36,35,36,37,37,40,44,45,41,38,35,35,35,0,0,0,0,0,0,0,0,0,0,0,0,2,2,1,0,0,0,0,0,0,1,4,0,0,5\n5: 26/09/2013,Thursday,34,36,50,56,49,37,37,35,36,36,39,56,59,46,42,38,35,34,35,0,0,1,1,1,0,1,0,0,0,0,2,1,5,1,0,0,0,0,0,3,4,5,1,0,13\n6: 27/09/2013,Friday,34,35,37,37,36,35,36,36,36,38,40,43,47,48,42,38,35,35,35,0,0,0,2,0,0,0,0,0,1,2,0,0,0,0,0,1,0,0,0,0,1,2,0,0,6\n7: 28/09/2013,Saturday,34,34,34,34,34,34,35,35,35,35,35,35,35,49,44,36,34,34,35,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,1\n8: 29/09/2013,Sunday,34,34,34,34,34,34,34,35,35,35,35,35,35,34,34,34,34,34,35,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1\n9: 30/09/2013,Monday,34,35,40,41,47,40,43,35,35,36,37,39,42,44,44,43,35,34,35,0,0,1,0,0,0,2,0,0,0,0,0,1,0,3,0,0,0,0,0,1,1,1,1,0,7\n10: 01/10/2013,Tuesday,35,36,40,41,40,36,36,35,36,36,37,41,44,47,41,37,35,35,35,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,3\n11: 02/10/2013,Wednesday,35,36,41,44,45,45,40,36,36,36,37,40,44,46,43,38,35,35,35,0,0,1,3,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,1,0,1,0,0,6\n12: 03/10/2013,Thursday,35,36,39,39,37,36,36,36,35,36,39,40,43,45,43,38,35,35,35,0,0,0,0,0,0,1,1,0,0,0,0,0,0,2,4,0,0,0,0,0,4,3,1,0,8\n13: 04/10/2013,Friday,35,35,36,36,36,36,36,36,36,39,43,44,45,45,43,39,35,35,35,0,0,0,0,0,0,0,0,0,1,0,0,0,1,2,0,0,0,0,0,0,1,1,0,0,4\n14: 05/10/2013,Saturday,34,34,34,34,34,35,35,35,35,35,35,35,35,35,35,35,34,35,36,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["#### ** (4b) Extracting fields relevant to the analysis **\n#### We will extract only those fields that will be useful for our further analysis in this lab.  Specifically, we are interested in field 2 (day), field 7 (commute time at 8 AM), and field 14 (commute time at 4 PM).  We consider only these 2 times since these best represent the morning and afternoon rush traffic.  Write a function `extractFields` that takes as input each record of `deerfootRDD` and produces a record for another RDD that only contains these 3 fields."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndef extractFields(deerfootRDDRecord):\n    \"\"\"Creates a record consisting of day, 8 AM commute time, and 4 PM commute time.\n\n    Args:\n        deerfootRDDRecord : a comma separated string consisting of all fields in the data set.\n\n    Returns:\n        extracted record: a comma separated record (day, 8 AM commute time, 4 PM commute time)\n    \"\"\"\n    split_record = deerfootRDDRecord.split(',')\n    return (split_record[1], int(split_record[6]), int(split_record[13]))\n   \nprint extractFields(deerfootRDD.take(1)[0])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(u&apos;Saturday&apos;, 35, 35)\n</div>"]}}],"execution_count":37},{"cell_type":"markdown","source":["#### ** (4c) Obtaining extracted RDD **\n#### Transform the `deerfootRDD` so that we get a resulting `deerfootPeakRDD` that only has peak hour commute times."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndeerfootPeakRDD = deerfootRDD.map(extractFields)\n\nprint deerfootPeakRDD.take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(u&apos;Saturday&apos;, 35, 35)]\n</div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["#### ** (4d) Obtaining stats - counting number of occurrences of each day of the week **\n#### Start with the `deerfootPeakRDD`.  Create a pair RDD `deerfootDayPairRDD` that contains records where day is the key and 1 is the value. Apply another transformation on `deerfootDayPairRDD` to get a `deerfootDayCounts` RDD"],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndeerfootDayPairRDD = deerfootPeakRDD.map(lambda day: (day[0], 1))\ndeerfootDayCounts = deerfootDayPairRDD.reduceByKey(lambda a, b: a + b)\n\ndeerfootDayCountsList = deerfootDayCounts.collect()\nprint deerfootDayCountsList\ndeerfootDayCountsDict = dict(deerfootDayCountsList)\nprint deerfootDayCountsDict.get('Friday')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(u&apos;Tuesday&apos;, 29), (u&apos;Sunday&apos;, 29), (u&apos;Friday&apos;, 28), (u&apos;Wednesday&apos;, 29), (u&apos;Saturday&apos;, 29), (u&apos;Thursday&apos;, 29), (u&apos;Monday&apos;, 29)]\n28\n</div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["#### ** (4e) Filtering out Saturdays and Sundays **\n#### As we can see from the previous result, there is almost an equal number of days of each type in the data set, which suggests that there is no big gap in the data collection.  Let's say we are now only interested in commute time stats for Monday to Friday.  Write a function called `filterSatSun` that filters out records for Saturdays and Sundays in `deerfootPeakRDD`.  Apply this transformation on `deerfootPeakRDD` to obtain an RDD called `deerfootPeakMFRDD`."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndef filterSatSun(deerfootPeakRDDRecord):\n    \"\"\"Ignores \"Saturday\" and \"Sunday\" records.\n\n    Args:\n        deerfootPeakRDDRecord: A comma separated string (day, 8 AM commute time, 4 PM commute time).\n\n    Returns:\n        false if day is \"Saturday\" or \"Sunday\". true if otherwise\n    \"\"\"\n    if deerfootPeakRDDRecord[0] == 'Saturday' or deerfootPeakRDDRecord[0] == 'Sunday':\n        return False\n    \n    return True\n\ndeerfootPeakMFRDD = deerfootPeakRDD.filter(filterSatSun)\nprint deerfootPeakMFRDD.take(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(u&apos;Monday&apos;, 45, 40), (u&apos;Tuesday&apos;, 52, 40), (u&apos;Wednesday&apos;, 39, 40), (u&apos;Thursday&apos;, 49, 56), (u&apos;Friday&apos;, 36, 43)]\n</div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["#### ** (4f) Computing average commute times for each day of the week **\n#### We will now compute the average of commute times for each day of the week for both 8 AM and 4 PM. To do this, first create a pair RDD called `deerfootPeakAMRDD` where each record has day as the key and 8 AM commute time as value.  Apply one or more appropriate transformations to compute average.  Repeat the process for the evening rush hour.  You can use the previously computed `deerfootDayCountsDict' in the average calculation."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndeerfootPeakAMRDD = deerfootPeakMFRDD.map(lambda data: (data[0], data[1]))\ndeerfootPeakAMreduceByDay = deerfootPeakAMRDD.reduceByKey(lambda a, b: a + b).collect()\nprint deerfootPeakAMreduceByDay\n\namAverages = list()\n\nfor item in deerfootPeakAMreduceByDay:\n    avg = item[1]/float(deerfootDayCountsDict.get(item[0]))\n    amAverages.append((item[0],avg))\n\ndeerfootPeakPMRDD = deerfootPeakMFRDD.map(lambda data: (data[0], data[2]))\ndeerfootPeakPMreduceByDay = deerfootPeakPMRDD.reduceByKey(lambda a, b: a + b).collect()\n\npmAverages = list()\n\nfor item in deerfootPeakPMreduceByDay:\n    avg = item[1]/float(deerfootDayCountsDict.get(item[0]))\n    pmAverages.append((item[0],avg))\n\nprint amAverages\nprint pmAverages"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(u&apos;Tuesday&apos;, 1290), (u&apos;Friday&apos;, 1080), (u&apos;Wednesday&apos;, 1282), (u&apos;Monday&apos;, 1231), (u&apos;Thursday&apos;, 1192)]\n[(u&apos;Tuesday&apos;, 44.48275862068966), (u&apos;Friday&apos;, 38.57142857142857), (u&apos;Wednesday&apos;, 44.206896551724135), (u&apos;Monday&apos;, 42.44827586206897), (u&apos;Thursday&apos;, 41.10344827586207)]\n[(u&apos;Tuesday&apos;, 41.44827586206897), (u&apos;Friday&apos;, 43.0), (u&apos;Wednesday&apos;, 40.93103448275862), (u&apos;Monday&apos;, 40.310344827586206), (u&apos;Thursday&apos;, 41.37931034482759)]\n</div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["#### ** (4g) Computing max morning hour rush commute times for each day of the week **\n#### For 8 AM, find the maximum commute time for each day of the week."],"metadata":{}},{"cell_type":"code","source":["# TODO: Replace <FILL IN> with appropriate code\ndeerfootPeakAMMaxreduceByDay = deerfootPeakAMRDD.reduceByKey(max).collect()\n\nfor item in deerfootPeakAMMaxreduceByDay:\n    print item\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(u&apos;Tuesday&apos;, 87)\n(u&apos;Friday&apos;, 57)\n(u&apos;Wednesday&apos;, 61)\n(u&apos;Monday&apos;, 64)\n(u&apos;Thursday&apos;, 57)\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.3","nbconvert_exporter":"python","file_extension":".py"},"name":"SENG501_lab1","notebookId":2329435937481639},"nbformat":4,"nbformat_minor":0}
